"""
Flask App for Hotel Cash Transaction Detection
Upload videos and extract clips around detected transactions
Uses the EXACT same detection logic as main.py
"""

from flask import Flask, render_template, request, jsonify, send_file, send_from_directory
from werkzeug.utils import secure_filename
from pathlib import Path
import os
import json
import threading
import uuid
from datetime import datetime
import cv2
from ultralytics import YOLO
import numpy as np
import math
import time
import subprocess
import shutil
try:
    import psutil
    PSUTIL_AVAILABLE = True
except ImportError:
    PSUTIL_AVAILABLE = False

# Import the detection classes
from main import SimpleHandTouchConfig, SimpleHandTouchDetector
from multi_detector import MultiEventDetector

app = Flask(__name__)

# Check if ffmpeg is available for video conversion
def check_ffmpeg():
    """Check if ffmpeg is available on the system"""
    return shutil.which('ffmpeg') is not None

FFMPEG_AVAILABLE = check_ffmpeg()
if FFMPEG_AVAILABLE:
    print("‚úÖ FFmpeg detected - will use for video conversion")
else:
    print("‚ö†Ô∏è  FFmpeg not found - using OpenCV codecs only")

def convert_avi_to_mp4(avi_path):
    """Convert AVI file to browser-compatible MP4 using ffmpeg"""
    if not FFMPEG_AVAILABLE:
        return None
    
    mp4_path = avi_path.replace('.avi', '.mp4')
    
    try:
        # Get original video FPS to preserve it
        cap = cv2.VideoCapture(avi_path)
        original_fps = cap.get(cv2.CAP_PROP_FPS)
        cap.release()
        
        # Use ffmpeg to convert with H.264 codec (browser compatible, smooth playback)
        cmd = [
            'ffmpeg',
            '-i', avi_path,
            '-c:v', 'libx264',    # H.264 video codec
            '-preset', 'medium',  # Better quality encoding
            '-crf', '20',         # Higher quality (lower = better, 20 is high quality)
            '-pix_fmt', 'yuv420p', # Ensure compatibility
            '-movflags', '+faststart',  # Enable streaming
            '-r', str(int(original_fps)),  # Preserve original FPS
            '-vsync', 'cfr',      # Constant frame rate (no drops)
            '-y',                 # Overwrite output file
            mp4_path
        ]
        
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=60)
        
        if result.returncode == 0 and os.path.exists(mp4_path):
            # Delete original AVI file
            try:
                os.remove(avi_path)
            except:
                pass
            return mp4_path
        else:
            print(f"‚ö†Ô∏è  FFmpeg conversion failed: {result.stderr}")
            return None
    except Exception as e:
        print(f"‚ö†Ô∏è  FFmpeg error: {e}")
        return None

# Load configuration from config.json
CONFIG_FILE = 'config.json'
try:
    with open(CONFIG_FILE, 'r', encoding='utf-8') as f:
        APP_CONFIG = json.load(f)
    print(f"‚úÖ Loaded configuration from {CONFIG_FILE}")
except FileNotFoundError:
    print(f"‚ö†Ô∏è  Config file not found, using defaults")
    APP_CONFIG = {
        'MAX_FILE_SIZE_MB': 500,
        'ALLOWED_EXTENSIONS': ['mp4', 'avi', 'mov', 'mkv'],
        'MAX_VIDEOS_PER_UPLOAD': 5,
        'SECONDS_BEFORE_TRANSACTION': 2,
        'SECONDS_AFTER_TRANSACTION': 2
    }

app.config['UPLOAD_FOLDER'] = 'uploads'
app.config['OUTPUT_FOLDER'] = 'outputs'
app.config['MAX_CONTENT_LENGTH'] = APP_CONFIG.get('MAX_FILE_SIZE_MB', 500) * 1024 * 1024
app.config['ALLOWED_EXTENSIONS'] = set(APP_CONFIG.get('ALLOWED_EXTENSIONS', ['mp4', 'avi', 'mov', 'mkv']))
app.config['MAX_VIDEOS'] = APP_CONFIG.get('MAX_VIDEOS_PER_UPLOAD', 5)

# Create necessary folders
Path(app.config['UPLOAD_FOLDER']).mkdir(exist_ok=True)
Path(app.config['OUTPUT_FOLDER']).mkdir(exist_ok=True)
Path('static').mkdir(exist_ok=True)

# Store processing status
processing_status = {}

print(f"üìÅ Upload folder: {app.config['UPLOAD_FOLDER']}")
print(f"üìÅ Output folder: {app.config['OUTPUT_FOLDER']}")
print(f"üìè Max file size: {APP_CONFIG.get('MAX_FILE_SIZE_MB', 500)}MB")
print(f"üé• Max videos per upload: {app.config['MAX_VIDEOS']}")
print()


def allowed_file(filename):
    return '.' in filename and filename.rsplit('.', 1)[1].lower() in app.config['ALLOWED_EXTENSIONS']


@app.errorhandler(413)
def request_entity_too_large(error):
    """Handle file too large errors"""
    max_size = APP_CONFIG.get('MAX_FILE_SIZE_MB', 500)
    return jsonify({
        'error': 'File too large',
        'message': f'ÌååÏùº ÌÅ¨Í∏∞Í∞Ä ÎÑàÎ¨¥ ÌÅΩÎãàÎã§. ÏµúÎåÄ ÌóàÏö© ÌÅ¨Í∏∞: {max_size}MB',
        'max_size_mb': max_size
    }), 413


class TransactionClipExtractor:
    """Extract clips around detected events (Cash, Fire, Violence) using multi-detector"""
    
    def __init__(self, config_dict=None, options=None):
        self.config_dict = config_dict
        self.options = options or {}
        
        # Debug options
        self.debug_velocity = self.options.get('debug_velocity', False)
        self.show_detection_count = self.options.get('show_detection_count', True)
        
        # Initialize multi-event detector
        self.detector = MultiEventDetector(config_dict)
        
        # Clip extraction settings from config
        self.SECONDS_BEFORE = config_dict.get('SECONDS_BEFORE_TRANSACTION', 2)
        self.SECONDS_AFTER = config_dict.get('SECONDS_AFTER_TRANSACTION', 2)
        self.MERGE_THRESHOLD = config_dict.get('MERGE_CLIPS_WITHIN_SECONDS', 0.5)
        
        self.detection_types = config_dict.get('DETECTION_TYPES', {})
        self.video_extension = '.avi'  # Will be set based on available codec
        
        print(f"‚è±Ô∏è  Clip padding: {self.SECONDS_BEFORE}s before, {self.SECONDS_AFTER}s after")
        print(f"üîó Merge clips within: {self.MERGE_THRESHOLD}s")
        if self.debug_velocity:
            print(f"üêõ Velocity visualization: ENABLED")
        else:
            print(f"üêõ Velocity visualization: DISABLED")
        if self.show_detection_count:
            print(f"üìä Detection counting: ENABLED")
        print()
    
    def detect_transactions(self, video_path, progress_callback=None):
        """Detect all events (cash, fire, violence) in video and save annotated version"""
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            return [], 0, None
        
        fps = int(cap.get(cv2.CAP_PROP_FPS))
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        
        # Handle time interval
        start_frame = 0
        end_frame = total_frames
        
        if not self.options.get('full_video', True):
            start_time = self.options.get('start_time', 0)
            end_time = self.options.get('end_time', None)
            
            start_frame = int(start_time * fps)
            if end_time:
                end_frame = min(int(end_time * fps), total_frames)
            else:
                end_frame = total_frames  # Process until end if no end_time specified
            
            # Seek to start frame
            cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)
        
        # Calculate actual frames to process
        frames_to_process = end_frame - start_frame
        duration_seconds = frames_to_process / fps
        
        # SKIP temporary video for large files - process on demand instead
        print(f"‚ö° Using on-demand processing (no temp video)")
        print(f"   This is faster for large videos and uses less disk space")
        temp_annotated_path = None  # No temp video
        self.video_extension = '.mp4'  # Will convert later
        
        # Reset cash detector state if it exists
        if 'CASH_EXCHANGE' in self.detector.detectors:
            cash_detector = self.detector.detectors['CASH_EXCHANGE']
            cash_detector.transaction_history = {}
            cash_detector.person_id_map = {}
            cash_detector.next_stable_id = 1
            cash_detector.cashier_persistence = {}
            cash_detector.stats = {
                'frames': 0, 
                'transactions': 0, 
                'confirmed_transactions': 0,
                'cash_detections': 0,
                'cash_types': {},
                'possible_detections': 0
            }
            # Reset possible detection tracking
            cash_detector.possible_events = []
            cash_detector.current_possible_event = None
        
        all_events = []  # Store all detected events
        current_events = {}  # Track ongoing events by type
        frame_num = start_frame  # Start from interval start
        
        # Process EVERY frame for smooth detection (no skipping)
        frame_skip = 1  # Process all frames for smooth output
        effective_frames = frames_to_process
        
        # Print analysis info
        if not self.options.get('full_video', True):
            print(f"‚è±Ô∏è  TIME INTERVAL: {start_frame/fps:.1f}s - {end_frame/fps:.1f}s")
            print(f"üìπ Analyzing INTERVAL: {frames_to_process} frames at {fps} FPS")
            print(f"‚è±Ô∏è  Interval duration: {duration_seconds/60:.1f} minutes")
        else:
            print(f"üìπ Analyzing FULL video: {frames_to_process} frames at {fps} FPS")
            print(f"‚è±Ô∏è  Estimated duration: {duration_seconds/60:.1f} minutes")
        
        print(f"üé¨ Processing ALL frames for smooth detection (no frame skipping)")
        
        # Debug counters
        hands_close_count = 0
        cash_seen_count = 0
        violence_count = 0
        fire_count = 0
        
        while True:
            ret, frame = cap.read()
            if not ret:
                current_frame_pos = int(cap.get(cv2.CAP_PROP_POS_FRAMES))
                if current_frame_pos < end_frame * 0.9:  # Less than 90% of target processed
                    print(f"\n‚ö†Ô∏è  Video reading stopped early at frame {current_frame_pos}/{end_frame}")
                break
            
            frame_num = int(cap.get(cv2.CAP_PROP_POS_FRAMES))
            
            # Stop if we've reached end_frame
            if frame_num >= end_frame:
                break
            
            # Progress update every 150 frames (every 10 seconds at 15fps)
            if progress_callback and frame_num % 150 == 0:
                frames_in_range = end_frame - start_frame
                progress_pct = int((frame_num - start_frame) / frames_in_range * 100)
                progress_callback(progress_pct)
                print(f"  ‚ö° Progress: {frame_num - start_frame}/{frames_in_range} frames ({progress_pct}%)")
            
            try:
                # Run detection (no need to save output_frame since we don't store temp video)
                _, detections_dict = self.detector.detect_all(frame, fps=fps, frame_number=frame_num)
            except Exception as e:
                print(f"‚ö†Ô∏è  Error processing frame {frame_num}: {e}")
                continue
            
            # Process each detection type
            for det_type, detections in detections_dict.items():
                if det_type == 'CASH_EXCHANGE':
                    # Handle cash transactions
                    if detections:
                        hands_close_count += 1
                    for trans in detections:
                        if trans.get('cash_detected'):
                            cash_seen_count += 1
                        
                        # Check if this is actually violence (fast movement or long duration)
                        is_violence = trans.get('is_violence', False)
                        
                        trans_key = f"{trans['p1_id']}-{trans['p2_id']}"
                        
                        # If violence flagged, treat as violence event
                        if is_violence:
                            violence_count += 1  # Count violence detections
                            event_key = f"VIOLENCE_{trans_key}"
                            if event_key not in current_events:
                                current_events[event_key] = {
                                    'type': 'VIOLENCE',
                                    'key': trans_key,
                                    'start_frame': frame_num,
                                    'end_frame': frame_num,
                                    'start_time': frame_num / fps,
                                    'end_time': frame_num / fps,
                                    'p1_id': trans['p1_id'],
                                    'p2_id': trans['p2_id'],
                                    'description': trans.get('cash_type', 'Fast Movement'),
                                    'velocities': trans.get('velocities', {})
                                }
                            else:
                                current_events[event_key]['end_frame'] = frame_num
                                current_events[event_key]['end_time'] = frame_num / fps
                        else:
                            # Normal cash transaction
                            event_key = f"CASH_{trans_key}"
                            
                            if event_key not in current_events:
                                current_events[event_key] = {
                                    'type': 'CASH_EXCHANGE',
                                    'key': trans_key,
                                    'start_frame': frame_num,
                                    'end_frame': frame_num,
                                    'start_time': frame_num / fps,
                                    'end_time': frame_num / fps,
                                    'p1_id': trans['p1_id'],
                                    'p2_id': trans['p2_id'],
                                    'hand_type': trans['hand_type']
                                }
                            else:
                                current_events[event_key]['end_frame'] = frame_num
                                current_events[event_key]['end_time'] = frame_num / fps
                                
                                # Check if event is too long (>5 seconds) - reclassify as violence
                                duration = current_events[event_key]['end_time'] - current_events[event_key]['start_time']
                                max_cash_duration = self.config_dict.get('MAX_CASH_TRANSACTION_SECONDS', 5)
                                if duration > max_cash_duration:
                                    print(f"  ‚ö†Ô∏è  üö® RECLASSIFYING as VIOLENCE: Transaction too long ({duration:.1f}s > {max_cash_duration}s)")
                                    # Change type to violence
                                    current_events[event_key]['type'] = 'VIOLENCE'
                                    current_events[event_key]['description'] = f'Extended interaction ({duration:.1f}s)'
                
                else:
                    # Handle violence/fire detections
                    for detection in detections:
                        # Count detections
                        if det_type == 'FIRE':
                            fire_count += 1
                        elif det_type == 'VIOLENCE':
                            violence_count += 1
                        
                        event_key = f"{det_type}_{frame_num}"
                        
                        if det_type not in current_events:
                            current_events[det_type] = {
                                'type': det_type,
                                'start_frame': frame_num,
                                'end_frame': frame_num,
                                'start_time': frame_num / fps,
                                'end_time': frame_num / fps,
                                'confidence': detection.get('confidence', 0.8),
                                'description': detection.get('description', det_type)
                            }
                        else:
                            # For VIOLENCE: if gap > 3 seconds, create NEW event (don't extend)
                            # This prevents merging separate violent incidents
                            if det_type == 'VIOLENCE':
                                last_frame = current_events[det_type]['end_frame']
                                gap_seconds = (frame_num - last_frame) / fps
                                if gap_seconds > 3.0:  # 3 second gap = new incident
                                    # Save current event and start new one
                                    all_events.append(current_events[det_type])
                                    current_events[det_type] = {
                                        'type': det_type,
                                        'start_frame': frame_num,
                                        'end_frame': frame_num,
                                        'start_time': frame_num / fps,
                                        'end_time': frame_num / fps,
                                        'confidence': detection.get('confidence', 0.8),
                                        'description': detection.get('description', det_type)
                                    }
                                else:
                                    # Continue existing event
                                    current_events[det_type]['end_frame'] = frame_num
                                    current_events[det_type]['end_time'] = frame_num / fps
                            else:
                                # Non-violence: just extend normally
                                current_events[det_type]['end_frame'] = frame_num
                                current_events[det_type]['end_time'] = frame_num / fps
            
            # Check for ended events (no detection in this frame)
            ended_events = []
            for event_key, event in list(current_events.items()):
                if event['end_frame'] < frame_num - 5:  # Event ended 5 frames ago
                    all_events.append(event)
                    ended_events.append(event_key)
            
            for event_key in ended_events:
                del current_events[event_key]
        
        # Add remaining active events
        for event in current_events.values():
            all_events.append(event)
        
        cap.release()
        
        # ‚úÖ EXTRACT VIOLENCE EVENTS from hand-touch possible detections
        # These are high-velocity hand movements detected by cash_detector
        print(f"\nüîç Checking {len(cash_detector.possible_events)} possible detections for violence...")
        violence_from_hands = 0
        for poss_event in cash_detector.possible_events:
            # Check if this possible event was flagged as violence
            if poss_event.get('is_violence', False) or poss_event.get('violence_detected', False):
                # Convert to final event format
                violence_event = {
                    'type': 'VIOLENCE',
                    'start_frame': poss_event['start_frame'],
                    'end_frame': poss_event['end_frame'],
                    'start_time': poss_event['start_frame'] / fps,
                    'end_time': poss_event['end_frame'] / fps,
                    'confidence': 1.0,
                    'description': poss_event.get('description', 'Violence (hand velocity)')
                }
                all_events.append(violence_event)
                violence_from_hands += 1
        
        if violence_from_hands > 0:
            print(f"  ‚úÖ Promoted {violence_from_hands} violence event(s) from hand-touch detections")
        
        # Merge close events
        merged_events = self._merge_close_events(all_events, fps)
        
        print(f"\n" + "="*70)
        print(f"üìä DETECTION SUMMARY REPORT")
        print(f"="*70)
        print(f"  Total frames analyzed: {frame_num - start_frame}")
        if self.show_detection_count:
            print(f"\nüîç FRAME-BY-FRAME DETECTIONS:")
            print(f"  üö® VIOLENCE detections: {violence_count} frames")
            print(f"  üî• FIRE detections: {fire_count} frames")
            print(f"  üíµ CASH detections: {cash_seen_count} frames")
            print(f"  ü§ù Hands close: {hands_close_count} frames")
        print(f"\n‚úÖ Detection complete: Found {len(merged_events)} event(s)")
        print(f"="*70)
        
        if len(merged_events) == 0:
            print(f"\n‚ö†Ô∏è  NO EVENTS DETECTED - Check 'Possible' folder for hand-close events")
            if hands_close_count == 0:
                print(f"     ‚ùå No hands detected close enough (<{cash_detector.config.HAND_TOUCH_DISTANCE}px)")
                print(f"     üí° Try: Increase HAND_TOUCH_DISTANCE in config")
            elif cash_seen_count == 0:
                print(f"     ‚ùå Hands were close but NO CASH/MATERIAL detected")
                print(f"     üí° Try: Set DETECT_CASH_COLOR to false (currently: {cash_detector.config.DETECT_CASH_COLOR})")
                print(f"     üí° Or: Lower CASH_DETECTION_CONFIDENCE (currently: {cash_detector.config.CASH_DETECTION_CONFIDENCE})")
            print()
        else:
            for event in merged_events:
                det_type = event.get('type', 'UNKNOWN')
                label = self.detection_types.get(det_type, {}).get('label', det_type)
                print(f"  ‚Ä¢ {label}: {event['start_time']:.1f}s - {event['end_time']:.1f}s")
        
        # Note: Possible detections will be saved later when we know the output folder location
        # (We can't save here because we don't know the job output folder yet)
        
        return merged_events, fps, temp_annotated_path
    
    def _merge_close_events(self, events, fps):
        """Merge events that are close together (same type)
        
        Violence events are NOT merged - each violent incident is a separate clip.
        Cash events within MERGE_THRESHOLD are merged (same transaction).
        """
        if not events:
            return []
        
        # Sort by type and start time
        events.sort(key=lambda x: (x.get('type', ''), x['start_time']))
        
        merged = []
        current = events[0].copy()
        
        for event in events[1:]:
            # NEVER merge violence events - each attack is separate
            if event.get('type') == 'VIOLENCE' or current.get('type') == 'VIOLENCE':
                merged.append(current)
                current = event.copy()
                continue
            
            # For non-violence: merge if same type and within threshold
            if (event.get('type') == current.get('type') and 
                event['start_time'] - current['end_time'] <= self.MERGE_THRESHOLD):
                current['end_frame'] = event['end_frame']
                current['end_time'] = event['end_time']
                # For cash, merge keys if different
                if 'key' in event and 'key' in current and event['key'] != current['key']:
                    current['key'] = f"{current['key']},{event['key']}"
            else:
                merged.append(current)
                current = event.copy()
        
        merged.append(current)
        return merged
    
    def _calculate_bbox_area(self, bbox):
        """Calculate area of bounding box in pixels"""
        if not bbox or len(bbox) < 4:
            return 0
        x1, y1, x2, y2 = bbox[:4]
        width = abs(x2 - x1)
        height = abs(y2 - y1)
        return int(width * height)
    
    def _merge_overlapping_clips(self, events, fps):
        """Merge events that would create overlapping clips
        
        Set MERGE_CLIPS_WITHIN_SECONDS to 0 in config.json to disable merging
        and export ALL detections as separate clips.
        """
        if not events:
            return []
        
        # Add margins and create clip ranges
        clip_ranges = []
        for event in events:
            start_frame = max(0, event['start_frame'] - int(self.SECONDS_BEFORE * fps))
            end_frame = event['end_frame'] + int(self.SECONDS_AFTER * fps)
            
            # Get priority (lower = higher priority)
            event_type = event.get('type', 'CASH_EXCHANGE')
            priority = self.detection_types.get(event_type, {}).get('priority', 99)
            
            clip_ranges.append({
                'start_frame': start_frame,
                'end_frame': end_frame,
                'start_time': start_frame / fps,
                'end_time': end_frame / fps,
                'type': event_type,
                'priority': priority,
                'p1_id': event.get('p1_id'),
                'p2_id': event.get('p2_id'),
                'hand_type': event.get('hand_type', 'N/A'),
                'confidence': event.get('confidence'),
                'description': event.get('description'),
                'events': [event]  # Track which events are in this clip
            })
        
        # Sort by priority first, then start frame
        clip_ranges.sort(key=lambda x: (x['priority'], x['start_frame']))
        
        # If MERGE_THRESHOLD is 0, return all clips separately (NO merging)
        if self.MERGE_THRESHOLD <= 0:
            print(f"  üìå NO MERGING: Exporting all {len(clip_ranges)} detections as separate clips")
            return clip_ranges
        
        # Merge overlapping ranges
        merged = []
        current = clip_ranges[0]
        
        for clip in clip_ranges[1:]:
            # Check if clips overlap or are very close
            overlap_threshold = int(self.MERGE_THRESHOLD * fps)
            
            if clip['start_frame'] <= current['end_frame'] + overlap_threshold:
                # Merge: extend current clip and keep highest priority
                current['end_frame'] = max(current['end_frame'], clip['end_frame'])
                current['end_time'] = current['end_frame'] / fps
                current['events'].extend(clip['events'])
                # Keep highest priority (lower number = higher priority)
                # Violence (priority 1) will override Cash (priority 3)
                if clip['priority'] < current['priority']:
                    old_type = current['type']
                    current['type'] = clip['type']
                    current['priority'] = clip['priority']
                    if old_type == 'CASH_EXCHANGE' and clip['type'] == 'VIOLENCE':
                        print(f"  üö® VIOLENCE OVERRIDE: Cash detection reclassified as VIOLENCE (priority 1 > 3)")
                print(f"  üîó Merging clips: {current['start_time']:.1f}s - {current['end_time']:.1f}s ({current['type']})")
            else:
                # No overlap: save current and start new
                merged.append(current)
                current = clip
        
        merged.append(current)
        
        print(f"  ‚úÖ Merged {len(clip_ranges)} detections into {len(merged)} clips")
        return merged
    
    def extract_clips(self, source_video_path, events, fps, output_folder, progress_callback=None):
        """Extract video clips by re-processing only needed frames"""
        # Open original source video
        cap = cv2.VideoCapture(source_video_path)
        if not cap.isOpened():
            return []
        
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        
        # Merge overlapping clips first
        merged_clips = self._merge_overlapping_clips(events, fps)
        
        clips = []
        
        for idx, clip_info in enumerate(merged_clips):
            start_frame = clip_info['start_frame']
            end_frame = clip_info['end_frame']
            start_time = clip_info['start_time']
            end_time = clip_info['end_time']
            
            # Create output path
            video_name = Path(source_video_path).stem
            event_type = clip_info.get('type', 'CASH_EXCHANGE')
            label = self.detection_types.get(event_type, {}).get('label', event_type)
            
            # Generate filename
            base_name = f"{video_name}_{event_type}"
            if event_type == 'CASH_EXCHANGE' and clip_info.get('p1_id'):
                base_name += f"_P{clip_info['p1_id']}_P{clip_info['p2_id']}"
            base_name += f"_{int(start_time)}s"
            clip_name_avi = base_name + '.avi'
            clip_path_avi = os.path.join(output_folder, clip_name_avi)
            
            # Use MJPEG AVI with high quality for smooth playback
            fourcc = cv2.VideoWriter_fourcc(*'MJPG')
            out = cv2.VideoWriter(clip_path_avi, fourcc, fps, (width, height))
            
            # Verify video writer was created successfully
            if not out.isOpened():
                # Try alternative codec
                print(f"  ‚ö†Ô∏è  MJPG codec failed, trying XVID...")
                fourcc = cv2.VideoWriter_fourcc(*'XVID')
                out = cv2.VideoWriter(clip_path_avi, fourcc, fps, (width, height))
            
            if not out.isOpened():
                print(f"‚ö†Ô∏è  Warning: Could not create video writer for clip {idx+1}")
                print(f"  Tried codecs: MJPG, XVID")
                print(f"  Target: {width}x{height} @ {fps}fps")
                continue
            
            print(f"  ‚úÖ Video writer ready: {width}x{height} @ {fps}fps")
            
            # Reset detector state
            if 'CASH_EXCHANGE' in self.detector.detectors:
                cash_detector = self.detector.detectors['CASH_EXCHANGE']
                cash_detector.transaction_history = {}
                cash_detector.person_id_map = {}
                cash_detector.next_stable_id = 1
                cash_detector.cashier_persistence = {}
            
            # Seek to start frame and process
            cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)
            
            frames_written = 0
            
            # ALWAYS collect comprehensive JSON data (not just debug mode)
            clip_json_data = {
                "clip_info": {
                    "filename": base_name,
                    "video_source": video_name,
                    "clip_number": idx + 1,
                    "total_clips": len(merged_clips),
                    "start_frame": start_frame,
                    "end_frame": end_frame,
                    "start_time_seconds": round(start_time, 2),
                    "end_time_seconds": round(end_time, 2),
                    "duration_seconds": round(end_time - start_time, 2),
                    "fps": fps,
                    "event_type": event_type,
                    "label": label,
                    "priority": clip_info.get('priority', 99)
                },
                "detection_summary": {
                    "merged_events_count": len(clip_info['events']),
                    "person_1_id": clip_info.get('p1_id'),
                    "person_2_id": clip_info.get('p2_id'),
                    "hand_type": clip_info.get('hand_type', 'N/A'),
                    "confidence": clip_info.get('confidence'),
                    "description": clip_info.get('description')
                },
                "score_interpretation": {
                    "geometric": {
                        "description": "Shape analysis (rectangular vs irregular)",
                        "card_indicator": "> 0.7 (rigid rectangle)",
                        "cash_indicator": "< 0.3 (bent/folded)"
                    },
                    "photometric": {
                        "description": "Glare/reflection detection",
                        "card_indicator": "> 0.5 (plastic reflection)",
                        "cash_indicator": "< 0.5 (matte paper)"
                    },
                    "chromatic": {
                        "description": "Color saturation analysis",
                        "cash_indicator": "> 0.6 (colorful bills)",
                        "card_indicator": "< 0.4 (gray/white)"
                    }
                },
                "violence_interpretation": {
                    "detection_methods": {
                        "velocity": "Fast hand movement (>25 px/f) + high acceleration (>20 px/f¬≤) indicates attack",
                        "yolo_model": "YOLO detects: fight, violence, weapon, knife, gun, assault",
                        "duration": "Violence must be continuous for >= 1 second"
                    },
                    "confidence_levels": {
                        "high": ">= 0.7 (70%) - Strong violence signal",
                        "medium": "0.5-0.7 - Possible violence",
                        "low": "< 0.5 - Weak signal, likely false positive"
                    },
                    "weapon_detection": ["knife", "gun", "blade", "pistol", "rifle", "sword", "bat", "weapon"]
                },
                "frames_with_detections": [],
                "frame_count": 0
            }
            
            for frame_idx in range(start_frame, end_frame):
                ret, frame = cap.read()
                if not ret:
                    print(f"  ‚ö†Ô∏è  Frame read failed at {frame_idx}, stopping clip")
                    break
                
                # Verify frame is valid
                if frame is None or frame.size == 0:
                    print(f"  ‚ö†Ô∏è  Invalid frame at {frame_idx}, skipping")
                    continue
                
                # Re-run detector on this frame to get annotations
                try:
                    annotated_frame, detections_dict = self.detector.detect_all(frame, fps=fps, frame_number=frame_idx)
                except Exception as e:
                    print(f"  ‚ö†Ô∏è  Detection error at frame {frame_idx}: {e}")
                    annotated_frame = frame  # Use original frame if detection fails
                
                # Collect comprehensive detection data from this frame
                frame_data = {
                    "frame_number": frame_idx,
                    "timestamp_seconds": round(frame_idx/fps, 2),
                    "detections": []
                }
                
                # Extract ALL detection data
                for det_type, detections in detections_dict.items():
                    if det_type == 'CASH_EXCHANGE' and detections:
                        for det in detections:
                            detection_entry = {
                                "type": det_type,
                                "persons": f"P{det.get('p1_id', '?')} ‚Üî P{det.get('p2_id', '?')}",
                                "hand_type": det.get('hand_type', 'Unknown'),
                                "distance_px": round(det.get('distance', 0), 1),
                                "material_detected": det.get('cash_detected', False),
                                "material_type": det.get('cash_type', 'None'),
                                "is_violence": det.get('is_violence', False)
                            }
                            
                            # Add analysis scores if available
                            if 'analysis_scores' in det:
                                scores = det['analysis_scores']
                                detection_entry["analysis_scores"] = {
                                    "geometric": round(scores.get('geometric', 0), 3),
                                    "photometric": round(scores.get('photometric', 0), 3),
                                    "chromatic": round(scores.get('chromatic', 0), 3)
                                }
                                
                                # Add interpretation
                                detection_entry["interpretation"] = {
                                    "shape": "Card-like" if scores.get('geometric', 0) > 0.7 else "Cash-like" if scores.get('geometric', 0) < 0.3 else "Neutral",
                                    "surface": "Shiny/Glare" if scores.get('photometric', 0) > 0.5 else "Matte",
                                    "color": "Colorful" if scores.get('chromatic', 0) > 0.6 else "Grayscale" if scores.get('chromatic', 0) < 0.4 else "Neutral"
                                }
                                
                                # Add full debug data if available
                                if 'debug_data' in scores:
                                    detection_entry["debug_data"] = scores['debug_data']
                            
                            # Add velocity data if available
                            if 'velocities' in det:
                                detection_entry["velocities"] = det['velocities']
                            
                            # Add violence-specific debugging if flagged as violence
                            if det.get('is_violence', False):
                                violence_scores = scores if isinstance(scores, dict) else {}
                                detection_entry["violence_detection"] = {
                                    "flagged_as_violence": True,
                                    "detection_method": violence_scores.get('method', 'Unknown'),
                                    "violence_confidence": round(violence_scores.get('violence_confidence', 0), 3),
                                    "velocity_px_per_frame": round(violence_scores.get('velocity', 0), 2),
                                    "velocity_threshold": 100,
                                    "reason": f"Fast movement detected (velocity > threshold)",
                                    "person_1_velocity": round(det.get('velocities', {}).get('cashier', 0), 3),
                                    "person_2_velocity": round(det.get('velocities', {}).get('customer', 0), 3)
                                }
                            
                            frame_data["detections"].append(detection_entry)
                    
                    elif det_type == 'VIOLENCE':
                        for det in detections:
                            detection_entry = {
                                "type": "VIOLENCE",
                                "confidence": round(det.get('confidence', 0), 3),
                                "description": det.get('description', 'VIOLENCE'),
                                "class_name": det.get('class_name', 'violence'),
                                "bbox": det.get('bbox', []),
                                "detection_method": "YOLO Model",
                                "debug_info": {
                                    "model_class": det.get('class_name', 'unknown'),
                                    "confidence_percent": round(det.get('confidence', 0) * 100, 1),
                                    "bbox_area_px": self._calculate_bbox_area(det.get('bbox', [])),
                                    "threshold_used": 0.7,
                                    "min_duration_required": "1 second",
                                    "weapon_keywords": ["knife", "gun", "blade", "pistol", "rifle", "weapon"],
                                    "violence_keywords": ["fight", "violence", "assault", "punch", "kick", "attack"]
                                }
                            }
                            frame_data["detections"].append(detection_entry)
                    
                    elif det_type == 'FIRE':
                        for det in detections:
                            detection_entry = {
                                "type": "FIRE",
                                "confidence": round(det.get('confidence', 0), 3),
                                "description": det.get('description', 'FIRE'),
                                "bbox": det.get('bbox', [])
                            }
                            frame_data["detections"].append(detection_entry)
                
                # Add frame data if there were detections
                if frame_data["detections"]:
                    clip_json_data["frames_with_detections"].append(frame_data)
                
                # Ensure frame is in correct format and size before writing
                if annotated_frame.shape[:2] != (height, width):
                    annotated_frame = cv2.resize(annotated_frame, (width, height))
                
                # Write annotated frame
                out.write(annotated_frame)
                frames_written += 1
            
            out.release()
            
            # Update final frame count
            clip_json_data["frame_count"] = frames_written
            clip_json_data["detection_frame_count"] = len(clip_json_data["frames_with_detections"])
            
            # Add note if no detections in frames (but clip was created)
            if len(clip_json_data["frames_with_detections"]) == 0:
                clip_json_data["note"] = "Clip created from merged events but re-processing found no detections (people may have moved)"
            
            # ALWAYS save comprehensive JSON data (for tracking and analysis)
            json_filename = base_name + '.json'
            json_path = os.path.join(output_folder, json_filename)
            try:
                with open(json_path, 'w', encoding='utf-8') as f:
                    json.dump(clip_json_data, f, indent=2, ensure_ascii=False)
                print(f"  üìä Saved tracking data: {json_filename}")
            except Exception as e:
                print(f"  ‚ö†Ô∏è  Failed to save JSON: {e}")
                import traceback
                traceback.print_exc()
            
            # Convert AVI to browser-compatible MP4 if ffmpeg is available
            final_path = clip_path_avi
            final_name = clip_name_avi
            if FFMPEG_AVAILABLE:
                print(f"  üîÑ Converting to MP4...")
                mp4_path = convert_avi_to_mp4(clip_path_avi)
                if mp4_path:
                    final_path = mp4_path
                    final_name = clip_name_avi.replace('.avi', '.mp4')
                    print(f"  ‚úÖ Converted to MP4")
                else:
                    print(f"  ‚ö†Ô∏è  MP4 conversion failed, keeping AVI")
            
            clips.append({
                'filename': final_name,
                'path': final_path,
                'start_time': round(start_time, 2),
                'end_time': round(end_time, 2),
                'duration': round(end_time - start_time, 2),
                'type': event_type,
                'label': label,
                'priority': clip_info.get('priority', 99),
                'p1_id': clip_info.get('p1_id'),
                'p2_id': clip_info.get('p2_id'),
                'hand_type': clip_info.get('hand_type', 'N/A'),
                'confidence': clip_info.get('confidence'),
                'description': clip_info.get('description'),
                'frames': frames_written,
                'merged_count': len(clip_info['events'])  # How many events merged
            })
            
            if progress_callback:
                progress_callback(int((idx + 1) / len(merged_clips) * 100))
            
            merge_info = f" (merged {len(clip_info['events'])} detections)" if len(clip_info['events']) > 1 else ""
            print(f"  üìé Clip {idx+1}: {label} - {final_name} ({frames_written} frames){merge_info}")
        
        cap.release()
        return clips


def process_video(job_id, video_path, video_filename, options=None):
    """Process video in background thread"""
    annotated_video_path = None
    if options is None:
        options = {}
    
    try:
        processing_status[job_id] = {
            'status': 'processing',
            'progress': 0,
            'stage': 'Initializing detector...',
            'filename': video_filename,
            'started': datetime.now().isoformat(),
            'options': options
        }
        
        # Create extractor with full config
        extractor = TransactionClipExtractor(APP_CONFIG, options)
        
        # Detect transactions (creates annotated video)
        time_range = ""
        if not options.get('full_video', True):
            start = options.get('start_time', 0)
            end = options.get('end_time', 'end')
            time_range = f" ({start}s - {end}s)"
        
        processing_status[job_id]['stage'] = f'Detecting transactions{time_range}...'
        
        def detection_progress(progress):
            # Detection is now the main task (90% of time), extraction is fast (10%)
            processing_status[job_id]['progress'] = int(progress * 0.9)  # 0-90%
        
        transactions, fps, annotated_video_path = extractor.detect_transactions(video_path, detection_progress)
        
        processing_status[job_id]['stage'] = f'Found {len(transactions)} transaction(s). Extracting clips...'
        processing_status[job_id]['transactions_count'] = len(transactions)
        
        # Create output folder (needed for possible detections even if no clips)
        output_folder = os.path.join(app.config['OUTPUT_FOLDER'], job_id)
        Path(output_folder).mkdir(exist_ok=True)
        
        # ALWAYS save possible detections (even if no confirmed transactions)
        if 'CASH_EXCHANGE' in extractor.detector.detectors:
            cash_detector = extractor.detector.detectors['CASH_EXCHANGE']
            if hasattr(cash_detector, 'possible_events') and (cash_detector.possible_events or cash_detector.current_possible_event):
                try:
                    print(f"\nüíæ Saving possible detections (JSON + Video clips) to output folder...")
                    cash_detector._save_possible_detections(video_path, output_folder, fps)
                    processing_status[job_id]['has_possible_detections'] = True
                    print(f"  ‚úÖ Possible detections saved with video clips")
                except Exception as e:
                    print(f"  ‚ö†Ô∏è  Failed to save possible detections: {e}")
                    import traceback
                    traceback.print_exc()
        
        if len(transactions) == 0:
            processing_status[job_id]['status'] = 'completed'
            processing_status[job_id]['progress'] = 100
            processing_status[job_id]['clips'] = []
            processing_status[job_id]['message'] = 'No transactions detected'
            return
        
        # Extract clips from original video (re-process frames on-demand)
        def extraction_progress(progress):
            processing_status[job_id]['progress'] = 90 + int(progress * 0.1)  # Last 10%
        
        # Use original video path instead of non-existent annotated path
        clips = extractor.extract_clips(video_path, transactions, fps, output_folder, extraction_progress)
        
        processing_status[job_id]['status'] = 'completed'
        processing_status[job_id]['progress'] = 100
        processing_status[job_id]['clips'] = clips
        processing_status[job_id]['completed'] = datetime.now().isoformat()
        
        print(f"‚úÖ Job {job_id} completed: {len(clips)} clips extracted")
        
    except Exception as e:
        processing_status[job_id]['status'] = 'error'
        processing_status[job_id]['error'] = str(e)
        processing_status[job_id]['progress'] = 0
        print(f"‚ùå Job {job_id} error: {e}")


@app.route('/')
def index():
    return render_template('index.html')


@app.route('/upload', methods=['POST'])
def upload_files():
    """Handle multiple video uploads"""
    if 'videos' not in request.files:
        return jsonify({'error': 'No videos provided'}), 400
    
    files = request.files.getlist('videos')
    
    max_videos = app.config.get('MAX_VIDEOS', 5)
    if len(files) > max_videos:
        return jsonify({'error': f'Maximum {max_videos} videos allowed'}), 400
    
    # Get time interval and debug options
    full_video_str = request.form.get('full_video', 'true')
    full_video = full_video_str.lower() == 'true'
    
    # Handle time inputs (only if not full video)
    start_time = None
    end_time = None
    if not full_video:
        start_time_str = request.form.get('start_time', '')
        end_time_str = request.form.get('end_time', '')
        
        if start_time_str:
            start_time = float(start_time_str)
        else:
            start_time = 0  # Default to start of video
        
        if end_time_str:
            end_time = float(end_time_str)
        else:
            end_time = None  # Default to end of video
    
    debug_velocity = request.form.get('debug_velocity', 'false').lower() == 'true'
    show_detection_count = request.form.get('show_detection_count', 'true').lower() == 'true'
    
    # Debug: Print received options
    print(f"\nüìã UPLOAD OPTIONS:")
    print(f"   full_video: {full_video_str} ‚Üí {full_video}")
    if not full_video:
        print(f"   start_time: {start_time}s")
        print(f"   end_time: {end_time}s (None = until end)")
    print(f"   debug_velocity: {debug_velocity}")
    print(f"   show_detection_count: {show_detection_count}\n")
    
    jobs = []
    
    for file in files:
        if file and allowed_file(file.filename):
            filename = secure_filename(file.filename)
            job_id = str(uuid.uuid4())
            
            # Save uploaded file
            upload_path = os.path.join(app.config['UPLOAD_FOLDER'], f"{job_id}_{filename}")
            file.save(upload_path)
            
            # Processing options
            process_options = {
                'full_video': full_video,
                'start_time': start_time,
                'end_time': end_time,
                'debug_velocity': debug_velocity,
                'show_detection_count': show_detection_count
            }
            
            # Start processing in background
            thread = threading.Thread(target=process_video, args=(job_id, upload_path, filename, process_options))
            thread.daemon = True
            thread.start()
            
            jobs.append({
                'job_id': job_id,
                'filename': filename,
                'options': process_options
            })
    
    return jsonify({'jobs': jobs})


@app.route('/status/<job_id>')
def get_status(job_id):
    """Get processing status"""
    if job_id not in processing_status:
        return jsonify({'error': 'Job not found'}), 404
    
    return jsonify(processing_status[job_id])


@app.route('/download/<job_id>/<filename>')
def download_clip(job_id, filename):
    """Download or stream extracted clip"""
    output_folder = os.path.join(app.config['OUTPUT_FOLDER'], job_id)
    
    # Determine MIME type based on file extension
    if filename.endswith('.mp4'):
        mimetype = 'video/mp4'
    elif filename.endswith('.avi'):
        mimetype = 'video/x-msvideo'
    elif filename.endswith('.webm'):
        mimetype = 'video/webm'
    elif filename.endswith('.json'):
        mimetype = 'application/json'
    else:
        mimetype = 'video/mp4'  # Default
    
    # Check if request wants to stream (for video player) or download
    # Video players send Range requests for streaming
    if request.args.get('stream') or request.headers.get('Range'):
        # Stream for browser video player (no attachment)
        return send_from_directory(output_folder, filename, 
                                   as_attachment=False,
                                   mimetype=mimetype)
    else:
        # Force download
        return send_from_directory(output_folder, filename, as_attachment=True)


@app.route('/results/<job_id>')
def view_results(job_id):
    """View results page"""
    if job_id not in processing_status:
        return "Job not found", 404
    
    return render_template('results.html', job_id=job_id, status=processing_status[job_id])


@app.route('/possible/<job_id>')
def list_possible_detections(job_id):
    """List possible detection files (JSON + video) for a job"""
    if job_id not in processing_status:
        return jsonify({'error': 'Job not found'}), 404
    
    possible_folder = os.path.join(app.config['OUTPUT_FOLDER'], job_id, 'Possible')
    
    if not os.path.exists(possible_folder):
        return jsonify({'possible_detections': []})
    
    # Group JSON and video files together
    detections = []
    for filename in os.listdir(possible_folder):
        if filename.endswith('.json'):
            file_path = os.path.join(possible_folder, filename)
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                # Find matching video file (same name but .mp4 or .avi)
                base_name = filename.replace('.json', '')
                video_file = None
                for ext in ['.mp4', '.avi']:
                    video_path = os.path.join(possible_folder, base_name + ext)
                    if os.path.exists(video_path):
                        video_file = base_name + ext
                        break
                
                detections.append({
                    'filename': filename,
                    'video_filename': video_file,
                    'group_id': data.get('group_id'),
                    'event_count': data.get('event_count'),
                    'time_range': data.get('time_range'),
                    'json_url': f'/download/{job_id}/Possible/{filename}',
                    'video_url': f'/download/{job_id}/Possible/{video_file}?stream=true' if video_file else None
                })
            except Exception as e:
                print(f"Error reading {filename}: {e}")
    
    # Sort by group_id
    detections.sort(key=lambda x: x.get('group_id', 0))
    
    return jsonify({'possible_detections': detections, 'total': len(detections)})


@app.route('/download/<job_id>/Possible/<filename>')
def download_possible_file(job_id, filename):
    """Download or stream a possible detection file (JSON or video)"""
    possible_folder = os.path.join(app.config['OUTPUT_FOLDER'], job_id, 'Possible')
    
    if not os.path.exists(os.path.join(possible_folder, filename)):
        return "File not found", 404
    
    # Determine MIME type
    if filename.endswith('.mp4'):
        mimetype = 'video/mp4'
    elif filename.endswith('.avi'):
        mimetype = 'video/x-msvideo'
    elif filename.endswith('.json'):
        mimetype = 'application/json'
    else:
        mimetype = 'application/octet-stream'
    
    # Stream video files, download JSON files
    if request.args.get('stream') or request.headers.get('Range') or filename.endswith('.mp4') or filename.endswith('.avi'):
        return send_from_directory(possible_folder, filename, 
                                   as_attachment=False,
                                   mimetype=mimetype)
    else:
        return send_from_directory(possible_folder, filename, as_attachment=True)


if __name__ == '__main__':
    print("\n" + "="*70)
    print("üöÄ Starting Flask Cash Transaction Detector")
    print("="*70)
    print("Using EXACT same detection logic as main.py")
    print("Open your browser: http://localhost:5000")
    print(f"üìè Max file size: {APP_CONFIG.get('MAX_FILE_SIZE_MB', 2048)}MB")
    print("="*70 + "\n")
    
    # Run with threaded=True for better handling of large uploads
    app.run(debug=True, host='0.0.0.0', port=5000, threaded=True)
